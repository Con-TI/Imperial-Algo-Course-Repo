{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38bf321f",
   "metadata": {},
   "source": [
    "# Some approximations (Mental maths)\n",
    "### Approximating the harmonic sums\n",
    "\n",
    "### Approximating general sums\n",
    "Integral approach\n",
    "\n",
    "Diagrammatic/graphical approach\n",
    "\n",
    "Some functions to keep in mind:\n",
    "- $x^{1/x}$\n",
    "- $\\frac{\\ln{x}}{x}$\n",
    "- \n",
    "\n",
    "Identifying if your approximation is an over or under estimate.\n",
    "\n",
    "### Approximating exponents, sine, cosine using taylor expansions\n",
    "\n",
    "\n",
    "### Some commonly used log values\n",
    "\n",
    "\n",
    "### Approximating the geometric average of two numbers\n",
    "\n",
    "\n",
    "### Approximating square roots\n",
    "\n",
    "\n",
    "### Approximating high inverse powers (1/5^x, 1/7^x, 1/9^x, 1/3^x)\n",
    "\n",
    "\n",
    "### Approximating the number of nth order powers of i fitting in a given range\n",
    "\n",
    "\n",
    "### Approximating the number of perfect squares, cubes fitting in a given range\n",
    "\n",
    "\n",
    "### Approximating arc lengths of known curves\n",
    "Trigonometric curves\n",
    "- $\\sin{x}$\n",
    "- $\\cos{x}$\n",
    "- $\\tan{x}$\n",
    "- $\\arcsin{x}$\n",
    "- $\\arccos{x}$\n",
    "- $\\arctan{x}$\n",
    "\n",
    "Polynomials:\n",
    "- $x$\n",
    "- $x^2$\n",
    "- $x^3$\n",
    "- $x^4$\n",
    "\n",
    "### Approximating coin related outcomes\n",
    "Coin walks\n",
    "\n",
    "Coin probabilities\n",
    "\n",
    "Dealing with extra conditions\n",
    "\n",
    "### Approximating dice related outcomes\n",
    "Dice sums\n",
    "\n",
    "Dice probabilities\n",
    "\n",
    "Dice rolls\n",
    "\n",
    "\n",
    "Dealing with extra conditions\n",
    "\n",
    "\n",
    "### Approximating the number of primes in a given range\n",
    "\n",
    "\n",
    "### Number of positive integer solutions of a tuple given constraints\n",
    "#### Useful ideas\n",
    "Stars and bars\n",
    "\n",
    "Diophantine constraints\n",
    "\n",
    "Inclusion exclusion principle\n",
    "\n",
    "Non-exhaustive question styles:\n",
    "- $(x,y,z)$ such that $x+y+z = n$\n",
    "- $(x,y,z)$ such that $x+ay+bz = n$\n",
    "- $(x,y,z)$ such that $x+ay+bz = n$ and $x_1>x>x_0$, $y_1>y>y_0$, $z_1>z>z_0$\n",
    "- $(x,y,z)$ such that $x+y+z = n$, $x+ay+bz = m$\n",
    "- $(x,y,z)$ such that $x+ay+bz = n$, $x$ odd $y$ divisible by $3$ and $z$ even\n",
    "- $(x,y,z)$ such that $gcd(x,y,z) = 1$\n",
    "- $(x,y,z)$ such that $x+y+z <= n$\n",
    "- $(x,y,z)$ such that $x + y + z = n$, $1/x + 1/y + 1/z = 1/n$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6e6440",
   "metadata": {},
   "source": [
    "# Some algebraic identities\n",
    "### Geometric and arithmetic sums\n",
    "\n",
    "### Product-sum identity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff27374",
   "metadata": {},
   "source": [
    "# Basic Probability concepts\n",
    "\n",
    "### Sample space, Sigma algebra, and Probability measures\n",
    "\n",
    "**Sample space ($\\Omega$):** The set of all possible outcomes.\n",
    "\n",
    "**Sigma algebras:** A sigma algebra $\\mathcal{F}$ is a set of subsets $A$ of $\\Omega$, each subset corresponding to an event (e.g. if the sample space is the set of all outcomes of 3 coin flips, an event could be all outcomes with an odd number of heads). They obey the following properties/axioms:\n",
    "- $\\emptyset \\in \\mathcal{F}$.\n",
    "- Closed under countable unions.\n",
    "- Closed under complements.\n",
    "\n",
    "**Probability measures (Kolmogorov's definition):** A probability measure is a function $\\mathbb{P}:\\mathcal{F}\\rightarrow[0,1]$. It obeys the following properties/axioms:\n",
    "- $\\mathbb{P}(A)\\geq 0 \\forall A$\n",
    "- $\\mathbb{P}(\\Omega) = 1$\n",
    "- The probability of the union of disjoint sets is the sum of their individual probabilities.\n",
    "\n",
    "Common definitions for probability measures:\n",
    "- **Discrete sample space** (finite or countable): $\\mathbb{P}(A):=\\sum_{\\omega\\in A}{p(\\omega)}$ where $p(\\cdot)$ is the probability mass function defined for each point in the sample space.\n",
    "- **Continuous sample space**: $\\mathbb{P}(A):=\\int_{A}{f(x)dx}$ where $f(x)$ is the probability density function defined for each point in the sample space.\n",
    "Note that in both cases, the $pmf$ and $pdf$ functions sum/integrate to 1 over the entire sample space.\n",
    "\n",
    "A **probability space** is a tuple $(\\Omega, \\mathcal{F}, \\mathbb{P})$ of the three above terms When we have a stochastic process (a random process that evolves over time), we typically define a **filtration** in addition to the 3 things above.\n",
    "\n",
    "A **filtration** is a set of sigma algebras that are increasing over time, i.e. $\\mathcal{F}_s\\subset\\mathcal{F}_t \\forall s \\leq t$. Each $\\mathcal{F}_t$ in the filtration represents all possible outcomes that could have occured by time $t$. The importance of specifying filtrations comes in when we consider whether we have all the information up till time $t$, less information than that (maybe a delay in info), or more (future foresight). If a random variable is **$\\mathcal{F}_t$ measurable** its value is determined by the process up till time $t$.\n",
    "\n",
    "### De Morgan's Laws\n",
    "- Complement of union: $(A\\cup B)^c=A^c \\cap B^c$\n",
    "- Complement of intersection: $(A\\cap B)^c=A^c \\cup B^c$\n",
    "\n",
    "### Inclusion-Exclusion principle\n",
    "Suppose $(A_i)_{i\\in\\mathcal{I}}$ are all subsets of some universal set like $\\Omega$. The **inclusion exclusion principle** states:\n",
    "\\begin{equation*}\n",
    "|\\cup_{i\\in\\mathcal{I}} A_i| = \\sum_{i}|A_i| - \\sum_{i<j}|A_i\\cap A_j| + ... + (-1)^{n+1}|\\cap_{i\\in\\mathcal{I}}A_1|\n",
    "\\end{equation*}\n",
    "\n",
    "For a probability measure based on the sizes of sets, we have:\n",
    "\\begin{equation*}\n",
    "\\mathbb{P}(\\cup_{i\\in\\mathcal{I}} A_i) = \\sum_{i}\\mathbb{P}(A_i) - \\sum_{i<j}\\mathbb{P}(A_i\\cap A_j) + ... + (-1)^{n+1}\\mathbb{P}(\\cap_{i\\in\\mathcal{I}}A_1)\n",
    "\\end{equation*}\n",
    "\n",
    "### Combinatorics\n",
    "Basic definitions:\n",
    "- **Combinations**: The number of ways to choose $k$ objects from $n$ total objects is given by $\\binom{n}{k} = \\frac{n!}{k!(n-k)!}$\n",
    "- **Permutations**: The number of ways to arrange $k$ objects is given by $k!$.\n",
    "- $k$ length sequence of objects from n choices without replacement: Suppose you had to make an array of length $k$ and for each entry you can choose among $n$ objects. You can make $n^k$ unique arrays.\n",
    "\n",
    "Some common combinatorics/permutations ideas:\n",
    "- The **classic ballot problem** is described as the following: Suppose you have party A and party B with $a$ votes and $b$ votes, all in some random order and $a>b$. What is the probability that party A is strictly ahead of party B at all times when counting votes? To solve this problem, we reframe it as a random walk (combinatorial perspective solution):\n",
    "    - Consider a grid with $a$ rows and $b$ columns. We start at the bottom left corner, and at every step move up if we get an A vote and right if we get a B vote. At the end of our path (counting all votes) we always end at the top right corner. \n",
    "    - If we draw the line $y=x$ on the grid, we want to find the number of ways we can traverse from the bottom left to the top right while staying strictly above the line.\n",
    "    - The total number of ways to traverse the grid is given by $\\binom{a+b}{a}$\n",
    "    - By the reflection principle, there is a bijection between the number of bad paths and the number of paths given $a-1$ votes and $b+1$ votes : $\\binom{a+b}{a-1}$\n",
    "\n",
    "    Thus (after some expanding) the final probability of the event is $\\frac{a-b}{a+b}$. \n",
    "    \n",
    "    Below are some variations of the classic ballot problem which are all solvable using the same reflection principle + grid visualization:\n",
    "    - With a loose inequality: Suppose now that the parties A and B have $a\\geq b$ votes respectively. What is the probability that party A is never below party B when counting votes? \n",
    "    (Answer: $\\frac{a-b+1}{a+1}$)\n",
    "    - Catalan Numbers/Dyck paths: Suppose we have $n$ votes for A and $n$ votes for B. How many ways can the votes be counted such that the number of votes for A is always at least equal to B? (Answer: $\\frac{1}{n+1}\\binom{2n}{n}$)\n",
    "    - With a multiple: What is the probability that party A is more than $r$ times ahead when counting votes (conditioned on $a>rb$)?  (Answer: $\\frac{a-rb}{a+b}$)\n",
    "    - With a multiple and loose inequality: What is the probability that party A is at least $r$ times ahead when counting votes (conditioned on $a\\geq rb$)? \n",
    "    (Answer: $\\frac{a-rb+1}{a+b}$)\n",
    "- **Stars and bars**: Suppose we have $n$ stars and we want to find the number of ways they can be split into $k$ boxes where each box contains some number of stars or None. This is equivalent to having n+k-1 empty spaces, and choosing k-1 of them to have bars and the rest to be filled with stars. I.e., the number of ways is given by $\\binom{n+k-1}{k-1}$.\n",
    "- **Derangements**: A derangement on $n$ objects is the number of ways to permute $n$ objects such that no object is in its original position. The number of ways to do this can be deduced via the inclusion exclusion principle. Let $A_i$ denote the event that the $i$th object is in its original position. Then the derangement on $n$ objects is $n!-|\\cup_i A_i|$. We can expand $|\\cup_i A_i|$ by the inclusion exclusion principle: $\\sum_{i}|A_i| - \\sum_{i<j}|A_i\\cap A_j| + ... + (-1)^{n+1}|\\cap_{i}A_1|=\\sum_i (-1)^{i+1}\\binom{n}{i}(n-i)!$\n",
    "- **Menage problem**: Suppose we have $n$ couples, i.e. $2n$ total people, such that they are all seated on a circlar table. How many ways are there for them to be positioned so that no two people belonging to the same couple are adjacent while the order is alternating (i.e. ...MFMF...)? This is similar to a circular equivalent to the derangements problem. We consider the total number of possible arrangements and subtract from it the number of ways to get at least one couple matching based on the inclusion exclusion principle\n",
    "    - The total number of arrangements is given by: $n!(n-1)!$ (Hint: Fix the position of one person, arrange the remaining men, arrange the remaining women)\n",
    "    - The number of arrangements with k couples together: $(n-1)!\\binom{n}{k}2^k(n-k)!$ (Hint: Consider the men's positions still being fixed first but then automatically assigning each $k$ coupled woman to either the left or right of their husband).\n",
    "\n",
    "    The rest of the formula then follows via expansion. \n",
    "- **Collision probability**: Suppose you have $m$ balls that you want to distribute randomly across $n$ bins. What is the probability that you have a bin with two or more balls? This problem is easily solvable by considering the probability of having a unique bin for each ball: $\\frac{n(n-1)...(n-m+1)}{n^m}$ with edge cases of $m=1$ which leads to probability 0 and $m>n$ which leads to probability 1 by the **pigeonhole principle**.\n",
    "\n",
    "Sidenote (More on **catalan numbers**): In ballot problem with n votes for both parties and a weak inequality, the solution we arrive at is what is known as the nth **catalan number**. Catalan numbers appear in many combinatorics problems, below are some cases where thinking about catalan numbers may be useful:\n",
    "- Number of ways you can order events where one can only occur after another occurs.\n",
    "- Number of ways to arrange connections to avoid crossings\n",
    "- Number of ways to form an object whose structure depends on binary splitting\n",
    "\n",
    "Some extra examples on catalan numbers:\n",
    "- Suppose you have $n$ processes that you specify a start and end for. You need to find the number of ways to order them such that the end of a process $i$ always comes after the start of process $i$. E.g. so start, end, start, end is valid for two processes and so would start, start, end, end.\n",
    "- Suppose you have $2n$ points that form $n$ chords in a circle. Find the number of ways to get no intersections.\n",
    "- Suppose you have $n$ nodes. Find the number of ways to construct a binary tree with $n$ nodes.\n",
    "- Suppose you have a random walk that starts at $y=0$ and ends at $y=0$ with $2n$ total steps. Find the number of random walks that stay strictly above the $y=0$ axis.\n",
    "\n",
    "### Random variables as functions, PMFs/PDFs and CDFs\n",
    "Random variables are functions $X:\\Omega\\rightarrow \\mathbb{R}$ is a from the sample space $\\Omega$ to the reals. The image of a random set is defined by ${X(\\omega);\\omega\\in\\Omega}$\n",
    "- A **discrete random variable** is one where the image of the random variable $X$ is countable (e.g. it maps to the natural numbers or a finite set). The probabilities associated with a discrete random variable X are defined by a **probability mass function** (pmf) $p_X$ where $\\mathbb{P}(X=k) = p_X(k)$ and $\\mathbb{P}(X\\in A) = \\sum_{a\\in A} p_X(a)$.\n",
    "- A **continuous random variable** (cdf) is one where the image of the random variable $X$ is uncountable (e.g. it maps to a closed interval [a,b]). The probabilities associated with a continuous random variable $X$ are defined by a **probability density function** (pdf) $f_X$ where $\\mathbb{P}(X\\in A)=\\int_A f_X(x) dx$. \n",
    "\n",
    "The **cumulative distribution function** $F_X(x)$ of a random variable $X$ describes the following probability : $\\mathbb{P}(X\\leq x)$. If $X$ is continuous, then the CDF and pdf have the following relationship: $\\frac{dF_X}{dx}=f_X$. \n",
    "\n",
    "When it comes to discrete variables, we can in fact differentiate the CDF as well which gives the **generalized PDF**. To do this, we introduce the **dirac delta function** $\\delta()$ which is defined by:\n",
    "\\begin{array}{rl}\n",
    "    \\forall x\\neq 0, \\displaystyle\\delta(x)=0 &\\text{and}&\\displaystyle\\int_{-\\infty}^{\\infty}\\delta(x)dx=1\n",
    "\\end{array}\n",
    "It has many interpretations, the one useful in our case being the \"gradient\" at 0 for the step function $u(x)$.\n",
    "\\begin{equation*}\n",
    "u(x)=\\begin{cases}\n",
    "1 & x\\geq 0\\\\\n",
    "0 & \\text{ otherwise}\n",
    "\\end{cases}\n",
    "\\end{equation*}\n",
    "We can rewrite the CDF of a discrete variable which can take on values x_k as $F_X(x) = \\sum_{x_k} p_X(x_k)u(x-x_k)$. Differentiating this, we get:\n",
    "\\begin{equation*}\n",
    "f_X(x) = \\sum_{x_k} p_X(x_k)\\delta(x-x_k)\n",
    "\\end{equation*}\n",
    "\n",
    "Two random variables $X$ and $Y$ are **independent** if the realization of $X$ does not impact the probabilities of the outcomes $Y$ and vice versa. In probability terms: $\\mathbb{P}(X\\in A,Y\\in B)=\\mathbb{P}(X\\in A)\\mathbb{P}(Y\\in B)$ where $A$ and $B$ are images in $X$ and $Y$ of the events corresponding to $A$ and $B$.\n",
    "\n",
    "**Mutual exclusion**: Two events are mutually exclusive if one cannot occur with the other (their union is equivalent to the union of disjoint sets).\n",
    "- Example problems: \n",
    "    - Points on a circular arc problem: Suppose you have n points on a circular arc. What is the probability of all $n$ points lying on the same semicircle? To solve this problem, we consider $n$ events $(E_i)_{1 \\leq i\\leq n}$ which represent the event that the semi circle starting from point $i$ going clockwise contains all the other $n-1$ points. Each of these events are mutually exclusive, so $\\mathbb{P}(\\cup E_i)=\\sum \\mathbb{P}(E_i) = \\frac{n}{2^{n-1}}$\n",
    "\n",
    "### Conditional Probability\n",
    "The joint probability of events $A$ and $B$ is denoted by $\\mathbb{P}(A\\cap B)$, that is the event of both occurring at the same time. Suppose you have n variables $(X_i)_{1\\leq i\\leq n}$. Their **joint probability function**, either a pmf $p_{X_1,..,X_n}()$ or pdf $f_{X_1,..,X_n}()$, describes the point probability of getting a specific realization $(x_1,..,x_n)$ of the random variables. They follow the same rules of non-negativity and normalization (i.e. sum/integrate to 1) as regular probability functions.\n",
    "\n",
    "Given a **joint probability function**, the marginal probability function $f_{X_i}$ (or $p_{X_i}$) of a specific random variable $X_i$ is the probability of a specific realisation $x_i$ regardless of the other variables' outcomes, i.e. $f_{X_i}=\\int f_{X_1,...,X_n} dx_1...dx_{i-1}dx_{i+1}...dx_n$. When considering multiple variables, they are considered **mutually independent** (different from **pairwise indepence**) if the joint probability of every possible subset of variables is the product of each corresponding marginal probability function.\n",
    "\n",
    "The **conditional probability** of an event $A$ occuring given an event $B$ has occurred is:\n",
    "\\begin{equation*}\n",
    "\\mathbb{P}(A|B) = \\frac{\\mathbb{P}(A\\cap B)}{\\mathbb{P}(B)}\n",
    "\\end{equation*}\n",
    "When two variables are independent, the conditional has no effect.\n",
    "\n",
    "**Bayes theorem** states that we have the following relationship between conditional probabilities:\n",
    "\\begin{equation*}\n",
    "\\mathbb{P}(A|B) = \\frac{\\mathbb{P}(B|A)\\mathbb{P}(A)}{\\mathbb{P}(B)}\n",
    "\\end{equation*}\n",
    "\n",
    "### Expectation and Variance\n",
    "**Expectation**: The expectation of a random variable $X$ is a probability weighted average of all the possible outcomes of $X$.\n",
    "- Discrete expectation: $\\mu=\\mathbb{E}(X)=\\sum_{x\\in Im(X)} x p_X(x)$\n",
    "- Continuous expectation: $\\mu=\\mathbb{E}(X)=\\int_{Im(X)} xf_X(x)dx$\n",
    "\n",
    "**Linearity of expectation**: For constants $a,b,c$:\n",
    "\\begin{equation*}\n",
    "\\mathbb{E}(aX+bY+c) = a\\mathbb{E}(X)+b\\mathbb{E}(Y)+c\n",
    "\\end{equation*}\n",
    "\n",
    "**Variance**: The variance of a random variable $X$ is the expected squared deviation from the mean. The square root of variance is called **standard deviation**.\n",
    "\\begin{equation*}\n",
    "    Var(X) = \\mathbb{E}[(X-\\mu)^2] = \\mathbb{E}(X^2)-[\\mathbb{E}(X)]^2\n",
    "\\end{equation*}\n",
    "- Discrete variance: $Var(X) = \\sum (x -\\mu)^2p_X(x)$\n",
    "- Continuous variance: $Var(X) = \\int (x -\\mu)^2f_X(x)dx$\n",
    "\n",
    "**Non-linearity of variance**: For constants $a,b$\n",
    "\\begin{equation*}\n",
    "Var(aX+b) = a^2Var(X)\n",
    "\\end{equation*}\n",
    "\n",
    "**Law of the unconscious statistician (LOTUS)**: Suppose you have a random variable $X$ and a random variable $Y$ defined as a function $g()$ of $X$. Then the expectation of $Y$ is given by \n",
    "\\begin{equation*}\n",
    "\\mathbb{E}(Y)=\\begin{cases}\\sum g(x)p_X(x) \\\\\\\\\n",
    "\\int g(x)f(x)dx\\end{cases}\n",
    "\\end{equation*}\n",
    "\n",
    "**Law of total expectation**: Given random variables $X$ and $Y$, we have the following expectation formula:\n",
    "\\begin{equation*}\n",
    "\\mathbb{E}(X) = \\begin{cases}\n",
    "\\sum \\mathbb{E}(X|Y=y)\\mathbb{P}(Y=y) \\\\\\\\\n",
    "\\int \\mathbb{E}(X|Y=y)\\mathbb{P}(Y=y)\n",
    "\\end{cases}\n",
    "\\end{equation*} \n",
    "\n",
    "**Law of total variance**: Given random variables $X$ and $Y$, we have the following variance formula:\n",
    "\\begin{equation*}\n",
    "Var(X) = \\mathbb{E}[Var(Y|X)] + Var(\\mathbb{E}(Y|X))\n",
    "\\end{equation*}\n",
    "\n",
    "**Popoviciu inequality on variance**: Suppose we have a random variable $X$ bounded between $a$ and $b$. Then we have the following:\n",
    "\\begin{equation*}\n",
    "0\\leq Var(X)\\leq \\frac{(b-a)^2}{4}\n",
    "\\end{equation*}\n",
    "The above is derivable by considering that the maximum variance occurs when $X$ only takes on exactly $a$ or $b$ with equal probability.\n",
    "\n",
    "**Tail integrals/sums**: Given a random variable $X$ that takes on non-negative values, its expectation is given by:\n",
    "\\begin{equation*}\n",
    "\\mathbb{E}(X)=\n",
    "\\begin{cases}\n",
    "\\sum_{x=0}^\\infty \\mathbb{P}(X>x)\\\\\\\\\n",
    "\\int_0^\\infty \\mathbb{P}(X>x)dx\n",
    "\\end{cases}\n",
    "\\end{equation*}\n",
    "\n",
    "**Indicator RVs** are random variables that take on either 0 or 1 to indicate the occurence of an event. These are useful in problems involving expectations since it can reduce a complicated task to a sum of probabilities.\n",
    "\n",
    "**Geometric sums** refers to the sum of geometric sequences which occurs in cases where you need to consider long paths or infinitely long paths, e.g. coin tosses.\n",
    "\\begin{array}{rl}\n",
    "S_n &= \\displaystyle \\frac{a_0(1-r^n)}{1-r}\\\\\\\\\n",
    "S_\\infty &= \\displaystyle\\frac{a_0}{1-r}\n",
    "\\end{array}\n",
    "\n",
    "Example problems:\n",
    "- Traditional:\n",
    "    - \n",
    "- Finance context:\n",
    "    - \n",
    "\n",
    "\n",
    "### Covariance and Correlation\n",
    "**Covariance**: The covariance between two random variables $X$ and $Y$ measures the linear dependence between the two and is defined by the following expectation:\n",
    "\\begin{equation*}\n",
    "Cov(X,Y) = \\mathbb{E}(XY) - \\mathbb{E}(X)\\mathbb{E}(Y)\n",
    "\\end{equation*}\n",
    "It can take on negative values. If $X$ and $Y$ are the same, then the above reduces to the regular variance formula. \n",
    "\n",
    "**Bilinearity of covariance**: Given constants $a,b,c,d,e$ and random variables $X,Y,Z$\n",
    "\\begin{equation*}\n",
    "Cov(aX+bY+c,dZ+e) = adCov(X,Z) + bdCov(Y,Z)\n",
    "\\end{equation*}\n",
    "\n",
    "**Variance-covariance relationship**: Given variables $X$ and $Y$ with constants $a$ and $b$ we have.\n",
    "\\begin{array}{rl}\n",
    "Var(aX+bY) = a^2Var(X) + b^2Var(Y) + 2abCov(X,Y)\n",
    "Var(\\sum a_iX_i) = a_i^2\\sum Var(X_i) + 2a_ia_j\\sum Cov(X_i,X_j) \n",
    "\\end{array}\n",
    "\n",
    "**Law of total covariance**:\n",
    "\\begin{equation*}\n",
    "Cov(X,Y) = \\mathbb{E}[Cov(X,Y)|Z]+Cov(\\mathbb{E}(X|Z),\\mathbb{E}(Y|Z))\n",
    "\\end{equation*}\n",
    "\n",
    "**Correlation**: The correlation between two random variables $X$ and $Y$ can be understood as the normalized covariance and is given by:\n",
    "\\begin{equation*}\n",
    "Corr(X,Y) = \\frac{Cov(X,Y)}{\\sqrt{Var(X)Var(Y)}} = \\frac{\\mathbb{E}(X-\\mu_X)\\mathbb{E}(Y-\\mu_Y)}{\\sigma_X\\sigma_Y}\n",
    "\\end{equation*}\n",
    "Correlation varies between $-1$ and $1$. Multiple variables are **equicorrelated** if they all have the same pairwise correlation. \n",
    "\n",
    "Independence and correlation/covariance:\n",
    "- If two variables $X$ and $Y$ are independent, then we have $Cov(X,Y)=Corr(X,Y)=0$. The converse is not true for covariance (i.e. zero covariance does not imply independence).\n",
    "- If two variables have perfect correlation, i.e. $|Corr(X,Y)|=1$, then we have that $Y=aX+b$.\n",
    "\n",
    "**Frechet bounds** If $X\\in[a,b]$ and $Y\\in[c,d]$, then we have:\n",
    "\\begin{equation*}\n",
    "\\frac{max(ac,bd)-\\mu_x\\mu_y}{\\sigma_x\\sigma_y}\\leq Corr(X,Y)\\leq \\frac{max(ad,cd)-\\mu_x\\mu_y}{\\sigma_x\\sigma_y}\n",
    "\\end{equation*}\n",
    "\n",
    "**Partial correlation**: The partial correlation between two variables $X$ and $Y$ controlling for a set of random variables $Z_1,..,Z_n$ is the correlation between $X$ and $Y$ after removing the linear effect of $Z_1,...,Z_n$.\n",
    "\\begin{equation*}\n",
    "\n",
    "\\end{equation*}\n",
    "\n",
    "Example problems:\n",
    "- Traditional:\n",
    "    - Minimum correlation $\\rho$ given three pairwise correlated variables (i.e. equicorrelated variables)\n",
    "    - Upper-lower bounds on a correlation matrix with off diagonals equal to $\\rho$ (i.e. an equicorrelated matrix)\n",
    "    - Maximum/minimum possible correlation between two linear combinations of random variables.\n",
    "    - Correlation between the minimum and maximum of two random variables\n",
    "    - Covariance matrix of a linear transformation of a random vector\n",
    "    - Maximum correlation between two variables of fixed variances\n",
    "\n",
    "- Finance setting:\n",
    "    - Correlation between two infinitely sized stock indices:\n",
    "    - Correlation of related returns on stocks\n",
    "    - min-max correlations between the returns on two stocks\n",
    "    - Variance of an equally weighted portfolio\n",
    "    - Correlation between two portfolios constructed using deterministic weights.\n",
    "    - Expected correlation between two randomly constructed portfolios.\n",
    "    - Correlation between two indices with overlapping stocks\n",
    "    - Correlation between two stocks with orthogonality\n",
    "\n",
    "### Limits for random variables (Not as important besides the last two theorems)\n",
    "\n",
    "Limit superior and limit inferior of events: Let $(A_i)_{i\\geq1}$ be a sequence of events. \n",
    "- The **limit superior** of events is the event that occurs infinitely often: $\\omega\\in\\limsup A_n\\text{ if }\\omega\\in A_k \\text{ for infinitely many } k$\n",
    "- The **limit inferior** of events is the event that occurs eventually always: $\\omega\\in\\liminf A_n\\text{ if }\\exists N; \\forall k\\geq N, \\omega \\in A_k$\n",
    "\\begin{array}{rl}\n",
    "\\limsup_{n\\rightarrow\\infty}A_n &= \\cap^{\\infty}_{n=1}\\cup^{\\infty}_{k=n}A_k\\\\\\\\\n",
    "\\liminf_{n\\rightarrow\\infty}A_n &= \\cup^{\\infty}_{n=1}\\cap^{\\infty}_{k=n}A_k\n",
    "\\end{array}\n",
    "\n",
    "Sequences of random variables: We have 3 main definitions for the convergence of sequences of random variables.\n",
    "- **Almost sure convergence**: A sequence of random variables $X_n$ almost surely converges to $X$ if $\\mathbb{P}({\\omega;\\lim_{n\\rightarrow\\infty}X_n(\\omega)=X(\\omega)})$. This is the \"strongest\" definition in that it implies the other two.\n",
    "- **Convergence in probability**: A sequence of random variables $X_n$ converges in probability to $X$ if $\\lim_{n\\rightarrow\\infty}\\mathbb{P}(|X-X_n|>\\epsilon)=0\\forall\\epsilon$. This is the second strongest definition as it implies convergence in distribution but not almost sure convergence.\n",
    "- **Convergence in distribution**: A sequence of random variables $X_n$ converges in distirbution to $X$ if $\\lim_{n\\rightarrow\\infty}F_{X_n}(x)=F_X(x)$ for all points $x$ where $F_X$ is continuous. This is the weakest definition of convergence.\n",
    "\n",
    "\n",
    "Application the sequence definitions for random variables:\n",
    "- **Laws of Large Numbers (LLN)**: The law of large numbers states that the average of many independent samples of a finite variance random variable converges to its expected value.\n",
    "    - Weak law (Convergence in probability): $\\lim_{n\\rightarrow\\infty}\\mathbb{P}(|\\bar{X}_n-\\mu|>\\epsilon)=0$\n",
    "    - Strong law (Converges almost surely): $\\mathbb{P}(\\lim_{n\\rightarrow\\infty}\\bar{X}_n=\\mu)=1$\n",
    "- **Central Limit Theorem (CLT)**: The central limit theorem states that the standardized mean ($\\sqrt{n}(\\bar{X}_n-\\mu)$) of iid samples of finite variance random variables converges in distribution to a standard normal random variable. This is an application of a convergence in distribution.\n",
    "\n",
    "### Common distributions\n",
    "**Uniform**: Denoted by $Unif(a,b)$. It is a random variable taking values in the range $[a,b]$ all with equal probability. Its pdf is given by $f(x)=\\frac{1}{b-a}$. The discrete equivalent of this is a discrete uniform random variable taking integers in the same range, all with equal probability. \n",
    "- pdf: $f(x)=\\frac{1}{b-a}$\n",
    "- Expectation: $\\frac{(b-a)}{2}$\n",
    "- Variance: $\\frac{(b-a)^2}{12}$\n",
    "- Some common extensions:\n",
    "    - Distribution of the sum of 2 uniform random variables (Triangular distribution)\n",
    "    - Distribution of the sum of n uniform random variables (Tends to the normal distribution)\n",
    "\n",
    "**Bernoulli**: Denoted by $Bern(p)$. It is a random variable that takes on the value 1 with probability of $p$ and 0 otherwise. It is a random variable representing an event with probability of success of $p$.\n",
    "- pmf: $p_X(1) = p, p_X(0) = 1-p$\n",
    "- Expectation: $p$\n",
    "- Variance: $p(1-p)$\n",
    "\n",
    "**Binomial**: Denoted by $Binom(n,p)$. It is a random variable that takes on integer values from $0$ up to $n$ and is equivalent to taking the sum of $n$ iid bernoulli random variables each with probability $p$. It describes the number of successful trials given n total trials with each succeeding with probability $p$. It is denoted by the pdf $p_X(x)=\\binom{n}{x}p^x(1-p)^{n-x}$.\n",
    "- pmf: $p_X(x)=\\binom{n}{x}p^x(1-p)^{n-x}$\n",
    "- Expectation: $np$\n",
    "- Variance: $np(1-p)$\n",
    "\n",
    "**Geometric**: Denoted by $Geom(p)$. It is a random variable taking on integer values in the range $[0,\\infty)$ and describes the number of trials of a bernoulli variable with probability p till the first success.\n",
    "- pmf: $p_X(x)=(1-p)^{x-1}p$ (the $x^{th}$ trial is the first success)\n",
    "- Expectation: $\\frac{1}{p}$\n",
    "- Variance: $\\frac{1-p}{p^2}$\n",
    "\n",
    "**Negative Binomial**: Denoted by $NB(r,p)$. It is a random variable taking on integer values in the range $[r,\\infty)$ and describes the number of trials of a bernoulli variable with probability p till the $r^{th}$ success.\n",
    "- pmf: $p_X(x)=\\binom{n-1}{r-1}(1-p)^{n-r}p^r$\n",
    "- Expectation: $\\frac{r}{p}$ \n",
    "- Variance: $\\frac{r(1-p)}{p^2}$\n",
    "\n",
    "**Hypergeometric**: Denoted by Hypergeom(N,K,n). It is a random variable taking on integer values in the range $[\\max(0,n-(N-K)),\\min(n,K)]$ and describes the number of successes when we sample $n$ times from a finite population without replacement. The population will be one that contains $K$ correct items and $N$ total items (i.e. K\\leq N).\n",
    "- pmf: $P(X=k)=\\frac{\\binom{K}{k}\\binom{N-K}{n-k}}{\\binom{N}{n}}$\n",
    "- Expectation: $\\frac{nK}{N}$\n",
    "- Variance: $\\frac{nK(N-K)(N-n)}{N^2(N-1)}$\n",
    "\n",
    "Note that for the hypergeometric expectation and variance formulae, you can derive it by considering X to be a sum of indicator random variables.\n",
    "\n",
    "**Poisson**: Denoted by $Poi(\\lambda)$. It is a random variable taking on integer values in the range $[0,\\infty)$ and describes the number of events occuring in a fixed interval.\n",
    "- pmf: $p_X(x)=\\frac{\\lambda^x}{x!}e^{-\\lambda}$ \n",
    "- Expectation: $\\lambda$\n",
    "- Variance: $\\lambda$\n",
    "\n",
    "**Exponential**: Denoted by $Exp(\\lambda)$. It is a random variable taking on continuous values in the range $[0,\\infty)$ and describes the waiting time between consecutive events of a poisson process with parameter $\\lambda t$. It is a special case of the gamma distribution.\n",
    "- pdf: $f_X(x) = \\lambda e^{-\\lambda x}$\n",
    "- Expectation: $\\frac{1}{\\lambda}$\n",
    "- Variance: $\\frac{1}{\\lambda^2}$\n",
    "\n",
    "**Gamma**: Denoted by $Gamma(\\alpha,\\theta)$, where $\\alpha$ is the shape parameter and $\\theta$ is the scale parameter. It ranges between $(0,\\infty)$. Note that sometimes $\\theta$ is substituted with a rate parameter $\\lambda = 1/\\theta$. If $\\alpha=k$ and $\\theta =1/\\lambda$, then the gamma variable represents the waiting time between $h^{th}$ and $(h+k)^{th}$ events. \n",
    "- pdf: $f_X(x)=\\frac{1}{\\Gamma(\\alpha)\\theta^\\alpha}x^{\\alpha-1}e^{-x/\\theta}$\n",
    "- Expectation: $\\alpha\\theta$\n",
    "- Variance: $\\alpha\\theta^2$\n",
    "\n",
    "**Normal**: Denoted by $\\mathcal{N}(\\mu,\\sigma^2)$ where $\\mu$ is the mean and $\\sigma^2$ is the variance. The standard normal distribution is given by $\\mathcal{N}(0,1)$.\n",
    "- pdf: $f_X(x)=\\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{(x-\\mu)^2}{2\\sigma}}$\n",
    "- Expectation: $\\mu$\n",
    "- Variance: $\\sigma$\n",
    "- Some common extensions:\n",
    "    - Bivariate normal: Denoted by $\\mathcal{N}((\\mu_1,\\mu_2),\\Sigma)$ where $\\Sigma$ is the covariance matrix of the random vector. It describes a random vector where the first and second entries are each normally distributed according to $\\mathcal{N}(\\mu_1,\\sigma_1^2)$ and $\\mathcal{N}(\\mu_2,\\sigma_2^2)$ and have correlation $\\rho$. The joint pdf of the two is given by:\n",
    "    \\begin{equation*}\n",
    "        f_{XY}(x,y) = \\frac{1}{2\\pi\\sigma_x\\sigma_y\\sqrt{1-\\rho^2}}\\exp\\left(-\\frac{1}{2(1-\\rho^2)}\\left(\\frac{(x-\\mu_x)^2}{\\sigma_x^2}+\\frac{(y-\\mu_y)^2}{\\sigma_y^2}-\\frac{2\\rho(x-\\mu_x)(y-\\mu_y)}{\\sigma_x\\sigma_y}\\right)\\right)\n",
    "    \\end{equation*}\n",
    "    - Multivariate normal: Denoted by $\\mathcal{N}(\\mu,\\Sigma)$ where $\\mu$ is the mean vector and $\\Sigma$ the covariance matrix. It describes a random vector of normal random variables with pairwise correlations $\\rho_{ij}$. The joint pdf of the two is given by:\n",
    "    \\begin{equation*}\n",
    "        f_X(x)=\\frac{1}{(2\\pi)^{n/2}|\\Sigma|^{1/2}}\\exp\\left(-\\frac{1}{2}(x-\\mu)^T\\Sigma^{-1}(x-\\mu)\\right)\n",
    "    \\end{equation*}\n",
    "- **Projection theorem**: The projection theorem for jointly normal RVs $Y$ and $X$ states that the conditional expectation of $Y$ given $X$ is the linear projection of $Y$ onto $X$. This also implies that $Y$ can be decomposed into two orthogonal components, the linear projection onto $X$ and the orthogonal (to $X$) residual.\n",
    "\n",
    "Chi-square: Denoted by $\\chi_k^2$, where $k>0$ is the degrees of freedom. It represents the sum of $k$ squared iid standard normal variables and takes on values $\\geq 0$.\n",
    "- pdf: $f_X(x)=\\frac{1}{2^{k/2}\\Gamma(k/2)x^{k/2-1}e^{-x/2}}$\n",
    "- Expectation: $k$\n",
    "- Variance: $2k$\n",
    "\n",
    "Student's-t: Denoted by $t_\\nu$ where $\\nu$ is the degrees of freedom. It represents the ratio of a standard normal variable and the square root of a scaled chi-square random variable with degree $\\nu$. It varies between $\\pm\\infty$ like the normal distribution.\n",
    "- Expectation: 0\n",
    "- Variance: $\\frac{\\nu}{\\nu-2}$ if $\\nu > 2$, otherwise $\\infty$.\n",
    "- Considerations: Typically $\\nu$ is greater than 1. In the case where $\\nu\\leq 1$, the expectation and variance become undefined.\n",
    "- Use cases: It is useful for the t-test, where you perform a hypothesis test for the mean using unknown variance.\n",
    "\n",
    "F-distribution: Denoted by $F(d_1,d_2)$, where $d_1$ and $d_2$ are degrees of freedom. It describes the ratio between two scaled chi-square random variables (divided by their degree).\n",
    "- Expectation: $\\frac{d_2}{d_2-2}$\n",
    "- Variance: $\\frac{2d_2^2(d_1+d_2-2)}{d_1(d_2-2)^2(d_2-4)}$\n",
    "- Use cases: It is useful for hypothesis testing of the relevance of dependent variables in linear regression.\n",
    "\n",
    "Beta: Denoted by $Beta(\\alpha,\\beta)$, where $\\alpha,\\beta>0$. It ranges between $[0,1]$.\n",
    "- pdf: $f_X(x)=\\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}x^{\\alpha-1}(1-x)^{\\beta-1}$\n",
    "- Expectation: \\frac{\\alpha}{\\alpha+\\beta}\n",
    "- Variance: \\frac{\\alpha\\beta}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)}\n",
    "- Common extensions:\n",
    "    - **Dirichlet distribution**: A random vector X has a dirichlet distribution with parameters $\\alpha_1,...,\\alpha_K$ if $X_i>0$, $sum_1^KX_i=1$ and if the joint pdf follows:\n",
    "    \\begin{equation*}\n",
    "    f(x_1,...,x_K)=\\frac{\\Gamma(\\sum \\alpha_i)}{\\prod \\Gamma(\\alpha_i)}\\prod_{i=1}^Kx_i^{\\alpha_i-1}\n",
    "    \\end{equation*}\n",
    "    and each of the marginal distributions are equal to a beta distribution. This has expectation $\\mathbb{E}(X_i)=\\frac{\\alpha_i}{\\sum \\alpha_j}$ and variance $Var(X_i)=\\frac{\\alpha_i(\\sum \\alpha_j - \\alpha_i)}{(\\sum \\alpha_j)^2((\\sum \\alpha_j) +1)}$\n",
    "\n",
    "Relations between the distributions: \n",
    "- Bernoulli and Binomial: $Bin(n,p) = \\sum^n_1 Bern(p)$\n",
    "- Geometric and Negative Binomial: $Geom(p)=NB(1,p)$.\n",
    "- Binomial and Hypergeometric: A binomial RV is the equivalent of a hypergeometric RV if sampling was done with replacement. Hence if we take $N$ to infinity while keeping $p=K/N$ the same, the hypergeometric RV becomes a binomial RV.\n",
    "- Poisson and Exponential and Gamma: Suppose we have a random variable denoting the number of events up to time $t$ distributed according to $Poi(\\lambda t)$. Then the waiting time between events, by the memoryless property, is equal to an exponential RV with parameter $\\lambda$ (derivable by considering $N(t)=0$). For the time between $k$ events, the distribution becomes a gamma distribution with parameters $k$ and $1/\\lambda$. \n",
    " \n",
    "$Y=\\sum_{i=1}^n X_i$ for $Y\\sim Gamma(n,1/\\lambda)$ and $X_i\\sim Exp(\\lambda)$ iid.\n",
    "- Poisson and Binomial: A poisson variable with $\\lambda=np$ is the equivalent of a binomial variable with parameters $n$ and $p$ when $n\\rightarrow\\infty$ and $p\\rightarrow 0$ while $\\lambda=np$ stays the same.\n",
    "- Normal and Chi-square: $\\chi_\\nu^2=\\sum_{i=1}^\\nu \\mathcal{N}(0,1)^2$\n",
    "- Student's t and normal and chi-square: $t_\\nu = \\frac{\\mathcal{N}(0,1)}{\\sqrt(\\chi_\\nu^2/\\nu)}$.\n",
    "- Chi-square and gamma: $\\chi_k^2=Gamma(k/2,2)$\n",
    "- Beta and gamma: $\\frac{Gamma(\\alpha,1)}{Gamma(\\alpha,1)+Gamma(\\beta,1)}=Beta(\\alpha,\\beta)$\n",
    "- Chi-square and beta:  $\\frac{\\chi_p^2}{\\chi_p^2+\\chi_q^2}=Beta(\\frac{p}{2},\\frac{q}{2})$\n",
    "\n",
    "Additivitity rules between independent RVs:\n",
    "- Normal: $N(\\mu_1,\\sigma_1^2)+N(\\mu_2,\\sigma_2^2)=N(\\mu_1+\\mu_2,\\sigma_1^2+\\sigma_2^2)$\n",
    "- Poisson: $Poi(\\lambda_1)+Poi(\\lambda_2)=Poi(\\lambda_1+\\lambda_2)$\n",
    "- Binomial: $Bin(n_1,p) + Bin(n_2,p) = Bin(n_1+n_2,p)$\n",
    "- Negative binomial: $NB(r_1,p)+NB(r_2,p)=NB(r_1+r_2,p)$\n",
    "- Exponential: $Exp(\\lambda) + Exp(\\lambda) = Gamma(2,\\lambda)$\n",
    "- Gamma: $Gamma(\\alpha_1,\\theta) + Gamma(\\alpha_2,\\theta) = Gamma(\\alpha_1+\\alpha_2,\\theta)$\n",
    "- Chi-square: $\\chi_p^2+\\chi_q^2=\\chi_{p+q}^2$\n",
    "\n",
    "Shapes of the distributions:\n",
    "- Normal: Symmetric about the mean (hence mean=median=mode)\n",
    "- Student's t: Symmetric about 0 with heavier tails than the normal.\n",
    "- Binomial: Symmetric for $p=0.5$, otherwise it is left or right skewed depending on if $p$ is less or greater than 0.5.\n",
    "- Beta: Symmetric for $\\alpha = \\beta$. If $\\beta<\\alpha$ it is left skewed, if $\\beta>\\alpha$ it is right skewed.\n",
    "- Gamma: Right-skewed.\n",
    "- Geometric: Right skewed.\n",
    "- Hypergeometric: Symmetric for $K/N=0.5$. Skewed left if proportion is less than 0.5, and right otherwise.\n",
    "- F distribution: Right skewed.\n",
    "- Exponential: Right skewed.\n",
    "- Poisson: Right skewed.\n",
    "- Chi-square: Right skewed.\n",
    "\n",
    "### Moments of random variables\n",
    "Kth raw moment: The $k^{th}$ **raw moment** of a random variable $X$ is the following expectation:\n",
    "\\begin{equation*}\n",
    "\\mathbb{E}(X^k)\n",
    "\\end{equation*}\n",
    "\n",
    "Kth central moment: The $k^{th}$ **central moment** of a random variable $X$ is the following expectation:\n",
    "\\begin{equation*}\n",
    "\\mathbb{E}[(X-\\mathbb{E}(X))^2]\n",
    "\\end{equation*}\n",
    "\n",
    "Skew: The **skew** is the 3rd central moment of a random variable. The closer it is to 0, the more symmetric the distribution is around the mean. For a positive skew, the RV's distribution is right-skewed, and vice versa for left skew.\n",
    "\n",
    "Kurtosis: The **kurtosis** is the 4th central moment of a random variable. This describes the shape of the tails of a distribution, where it approaches 0 for those with tails similar to a normal distirbution. Otherwise there exists excess kurtosis, resulting in heavy tails if kurtosis is greater than 0 (leptokurtic) and light tails otherwise (platykurtic).\n",
    "\n",
    "MGF: The **moment generating function** of an RV is given by\n",
    "\\begin{equation*}\n",
    "M_X(t)=\\mathbb{E}(e^{tX})=\\sum_{k=0}^{\\infty}\\frac{t^k}{k!}\\mathbb{X^k}\n",
    "\\end{equation*}\n",
    "From the definition above, it is easy to see that the $k^{th}$ moment is the $k^{th}$ derivative of the MGF at $t=0$, hence why it is called moment generating.\n",
    "\n",
    "PGF: ...\n",
    "\n",
    "### Order statistics\n",
    "Suppose you have a random sample $X_1$,...,$X_n$ of size $n$, then we can order them based on value to yield $X_{(1)}\\leq...\\leq X_{(n)}$ where $X_(k)$ is the $k^{th}$ **order statistic**. The first order statistic is equivalent to the minimum and the nth the maximum (the median is given by $X_{(\\lceil n/2\\rceil)}$).\n",
    "\n",
    "The pdf of the k-th order statistic is given by:\n",
    "\\begin{equation*}\n",
    "f_{X_(k)}(x) = \\binom{n-1}{k-1}[F(x)]^{k-1}[1-F(x)]^{n-k}f(x), x\\in\\mathbb{R} \n",
    "\\end{equation*}\n",
    "The above formula can be understood as the product of the probability that there are $k-1$ values below x (given by $[F(x)]^{k-1}$), the probability that there are n-k values above (given by $[1-F(x)]^{n-k}$), the number of ways to choose $k-1$ points out of the remaining $n-1$ points.\n",
    "\n",
    "For the minimum and maximum order statistics, the formula reduces to the following:\n",
    "\\begin{array}{rl}\n",
    "f_{X_{(1)}}(x)&=n[1-F(x)]^{n-1}f(x) \\\\ \n",
    "f_{X_{(n)}}(x)&=n[F(x)]^{n-1}f(x)\n",
    "\\end{array}\n",
    "\n",
    "The joint distribution of the order statistics is given by:\n",
    "\\begin{equation*}\n",
    "f_{X_{(1)},...,X_{(n)}}(x_1,...,x_n)=n!f(x_1)...f(x_n), x_1<...<x_n\n",
    "\\end{equation*}\n",
    "\n",
    "**Uniform order statistics** (i.e. iid $Unif(0,1)$):\n",
    "- distribution of the kth order statistic: $X_{(k)}\\sim Beta(k,n-k+1)$\n",
    "- pdf of the kth order statistic: $f_{X_{(k)}}(x)=\\binom{n-1}{k-1}x^{k-1}(1-x)^{n-k}$\n",
    "- expected value of the kth order statistic: $\\frac{k}{n+1}$\n",
    "- joint pdf: $f_{X_{(1)},...,X_{(n)}}(x_1,..,x_n)=n!$\n",
    "\n",
    "Spacings: Consider the difference between consecutive uniform order statistics $D_k = X_{(k+1)}-X_(k)$ where $X_{(0)}=0,X_{(n+1)}=1$. I.e. $D_0=X_{(1)}$ and $D_n=1-X_{(n)}$. \n",
    "- distribution of spacings: $(D_0,...,D_n)\\sim Dirichlet(1,1...,1)$ with $D_k\\sim Beta(1,n)$\n",
    "- pdf of any spacing: $n!(1-x)^{n-1}$\n",
    "- joint pdf of the spacings: $f(x_1,...,x_K)=(n-1)!$\n",
    "- expected value of a spacing: $\\mathbb{E}(D_k)=\\frac{1}{n+1}$\n",
    "\n",
    "Order statistics on spacings: \n",
    "- Smallest spacing pdf: $f(x)=n(n-1)(1-nx)^{n-2}$\n",
    "- Smallest spacing expected value: $\\frac{1}{(n+1)^2}$\n",
    "- Largest spacing pdf: $f(x)=\\sum_{j=1}^n(-1)^{j-1}\\binom{n}{j}(n-1)(j)(1-jx)^{n-2}$\n",
    "- Largest spacing expected value: $\\frac{H_{n+1}}{n+1}$ where $H_i$ is the $i^{th}$ harmonic sum.\n",
    "\n",
    "Sidenote: Everything above is entirely derivable from first principles (i.e. just start from definitions of uniform random variables and order statistics and work your way down the derivations).\n",
    "\n",
    "Example problems:\n",
    "- Traditional:\n",
    "    - Probability and expected size of minimum/maximum of n uniform random variables\n",
    "    - Probability and expected size of median of n uniform random variables\n",
    "    - Probability and expected size of quantiles of n uniform random variables\n",
    "    - Probability and Expected value of the kth uniform order statistic\n",
    "    - Expected size of the smallest/largest section of a stick broken at n points\n",
    "    - Probability of a triangle given 2 breakpoints on a stick\n",
    "    - Skyline problem:\n",
    "    - Probability that at least k uniform points fall in an interval (binomial and beta distribution perspectives).\n",
    "- Finance context:\n",
    "    - Minimum loss \n",
    "    - Median return\n",
    "    - Maximum return\n",
    "    - Expected kth largest return\n",
    "    - Expected maximum drawdown\n",
    "    - Expected percentile returns\n",
    "    - Expected number of assets exceeding the median\n",
    "\n",
    "### Vector/Matrices extensions and definitions\n",
    "Stochastic matrices: Matrices whose rows and columns each sum to one and whose entries are greater than or equal to 0.\n",
    "\n",
    "Positive definite matrices: Symmetric matrices such that $\\forall x, x^TMx>0$\n",
    "- Properties:\n",
    "    - \n",
    "    - \n",
    "\n",
    "Positive-semi definite matrices: Symmetric matrices such that $\\forall x, x^TMx\\geq0$\n",
    "- Properties:\n",
    "    - \n",
    "    - \n",
    "\n",
    "Correlation matrix:\n",
    "\n",
    "Covariance matrix: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60895c90",
   "metadata": {},
   "source": [
    "# Important Inequalities\n",
    "\n",
    "**Markov**\n",
    "\n",
    "**Chebyshev**\n",
    "\n",
    "**Convexity and concavity**\n",
    "\n",
    "**Jensen's inequality**\n",
    "\n",
    "**Triangle inequality**\n",
    "\n",
    "**Cauchy Schwarz inequality**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623fd089",
   "metadata": {},
   "source": [
    "# Lin alg\n",
    "\n",
    "### Independent vectors, Span, Basis, Dimensionality\n",
    "\n",
    "### Dot product, Norm, Orthogonality, Gram Schmidt procedure\n",
    "\n",
    "### Linearity and matrices as linear transformations\n",
    "\n",
    "### Matrix subspaces : row space, column space, image, null space, rank-nullity\n",
    "\n",
    "### Determinants, Cofactors, Adjugates, Cramer's rule\n",
    "\n",
    "### Transpose, Inverse\n",
    "\n",
    "### Spectral theory: \n",
    "Eigen values, \n",
    "eigen vectors, \n",
    "characteristic polynomial, \n",
    "multiplicities, \n",
    "diagonalization\n",
    "\n",
    "### Positive definite and positive demi-definite matrices\n",
    "\n",
    "### Special square matrices and properties\n",
    "Symmetric matrices \n",
    "Triangular matrices\n",
    "Skew symmetric\n",
    "\n",
    "### Other Matrix decomposition (Besides eigenvalue decomposition)\n",
    "LU\n",
    "QR\n",
    "PLU\n",
    "Cholesky\n",
    "SVD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708f4b9d",
   "metadata": {},
   "source": [
    "# Matrix Calculus\n",
    "### Gradient, Divergence and curl\n",
    "\n",
    "### Hessian\n",
    "\n",
    "### Jacobian\n",
    "\n",
    "### Laplacian\n",
    "\n",
    "### Differentiation rules\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe2d5a5",
   "metadata": {},
   "source": [
    "# Linear Recurrence relations\n",
    "Recurrence relations often appear in markov chain related problems among others and hence knowing the methods behind solving specific types of them are useful to know.\n",
    "\n",
    "### Definition\n",
    "A **linear recurrence relation** defines each term as a combination of previous terms. It has general form:\n",
    "\\begin{equation*}\n",
    "x_n = a_1x_{n-1} + a_2x_{n-2}+...+a_kx_{n-k} + f(n)\n",
    "\\end{equation*}\n",
    "where $a_1, a_2,...,a_k$ are coefficients and $f(n)$ is the non-recursive term. \n",
    "\n",
    "Recurrence relations are classified based on three main things:\n",
    "- Homogeneity: If $f(n) = 0$, the recurrence relation is homogeneous. Otherwise it is not.\n",
    "- Constant vs Variable coefficients: if $a_i$ do not vary with $n$ then the coefficients are constant.\n",
    "- Order: A kth-order relation means that $x_n$ is a function of $x_{n-1}$ up to $x_{n-k}$.\n",
    "\n",
    "### Solving constant, homogeneous linear recurrences:\n",
    "Given the kth-order relation\n",
    "\\begin{equation*}\n",
    "x_n = a_1x_{n-1} + a_2x_{n-2}+...+a_kx_{n-k}\n",
    "\\end{equation*}\n",
    "we have the following **characteristic polynomial**\n",
    "\\begin{equation*}\n",
    "r^k = a_1r^{k-1} + a_2r^{k-2}+...+a_k\n",
    "\\end{equation*}\n",
    "\n",
    "We will denote x^{(h)}_n as the general solution to the homogeneous system. We have three cases of solutions to the characteristic polynomial:\n",
    "#### Case 1:\n",
    "If the characteristic equation has $k$ distinct roots $r_1, r_2,...,r_k$, then the general solution is given by:\n",
    "\\begin{equation*}\n",
    "x_n=\\sum_{j=1}^k{A_jr_j^n}\n",
    "\\end{equation*}\n",
    "where substituting the initial conditions provides the values of the unknown coefficients $A_1,...,A_k$.\n",
    "\n",
    "#### Case 2:\n",
    "If the characteristic equation has repeated roots, then given $r_1,...,r_p$ with multiplicities $m_1,..,m_p$ that sum to $k$, we have the general solution:\n",
    "\\begin{equation*}\n",
    "x_n=\\sum_{j=1}^p\\sum_{i=0}^{m_j-1}A_{j,i}n^ir^n_j=A_{1,0}r^n_1+..+A_{1,m_1-1}n^{m_1-1}r^n_1+A_{2,0}r^2_j...+A_{p,m_p-1}n^{m_p-1}r^n_p\n",
    "\\end{equation*}\n",
    "\n",
    "#### Case 3:\n",
    "If the characteristic equation has complex roots, then each of the complex conjugate pairs $\\alpha\\pm i\\beta$ contributes the following term to the general solution (letting $r$ denote $\\alpha+i\\beta$):\n",
    "\\begin{equation*}\n",
    "x_n=...+|r|^n(B\\cos{(n \\arg(r))}+C\\sin{(n \\arg(r))})\n",
    "\\end{equation*}\n",
    "where $|r|^2=\\alpha^2+\\beta^2$ and $\\arg(r)=\\arctan{\\beta/\\alpha}$.\n",
    "\n",
    "### Inhomogeneous linear recurrences\n",
    "In general, the solution to an inhomogeneous system is given by: $x_n^{GS}=x_n^{(h)}+x_n^{(p)}$ where $x^{(h)}_n$ denotes the homogeneous solution and x_n^{(p)} the particular solution. So the main extension is to solve the particular solution. This can typically be done by **Ansatz** which is the process of guessing a solution form (e.g. $x_n=A_0+A_1n+A_2n^2$) based on what $f(n)$ looks like then substituting to solve for the coefficients.\n",
    "\n",
    "### Green's function approach for linear recurrences:\n",
    "Green's function $G$ is the impulse response of an inhomogeneous linear differential operator $L$ (ODEs/Difference equations/Recurrence relations/etc.).\n",
    "\\begin{equation*}\n",
    "LG=\\delta\n",
    "\\end{equation*}\n",
    "where $\\delta$ is the dirac delta function (defined by $\\int_{-k}^k\\delta(x)dx=1, \\forall k$).\n",
    "\n",
    "In the case of a linear recurrence relation, we have the equation $Lx_n = f(n)$ that gives us:\n",
    "\\begin{equation*}\n",
    "LG(n,m) = \\delta_{n,m} = \\begin{cases} 1 & \\text{if } n=m \\\\ 0 & \\text{otherwise} \\end{cases}\n",
    "\\end{equation*}\n",
    "where $\\delta_{n,m}$ is the discrete dirac delta. The solution to the linear recurrence is then given by:\n",
    "\\begin{equation*}\n",
    "x_n = \\sum_{m}G(n,m)f(m)\n",
    "\\end{equation*}\n",
    "\n",
    "Note that when solving for $G(n,m)$, it may be necessary to specify two equations for $G$ depending on $n<m$, $n=m$ and $n>m$ and to assume continuity at $n=m$. The solution to the equation $LG(n,m) = \\delta_{n,m}$ is then just a matter of solving a homogeneous linear recurrence relation.\n",
    "\n",
    "### Extension:\n",
    "Methods for solving linear recurrence relations have many analogues in linear ODE/PDE systems and linear difference equations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0e9d6c",
   "metadata": {},
   "source": [
    "# Concepts in Random walks\n",
    "\n",
    "### Markov Property\n",
    "\n",
    "### Martingales\n",
    "\n",
    "### Stopping time and the Optional Stopping Theorem\n",
    "\n",
    "### Mean reverting walks\n",
    "\n",
    "### Random walk on a graph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe8aa1f",
   "metadata": {},
   "source": [
    "# Markov chains [[src](https://www.statslab.cam.ac.uk/~rrw1/markov/M.pdf)]\n",
    "\n",
    "### Definition\n",
    "For a markov chain, we have the following components:\n",
    "- The **probability space** $(\\Omega, \\mathcal{F}, \\mathbb{P})$.\n",
    "- A countable set $\\mathcal{I}$ which denotes our set of possible states in the **state-space**\n",
    "- The sequence of random variables $X_0, X_1,...\\in \\mathcal{I}$ representing our **markov chain** $(X_n)_{n\\geq 0}$. This can be understood as the path the random process takes\n",
    "- A row vector $\\lambda$ representing the initial distirbution over $\\mathcal{I}$ (i.e. $\\mathbb{P}(X_0=i_0)=\\lambda_i \\forall i\\in\\mathcal{I}$).\n",
    "- A stochastic matrix (meaning all entries are $\\geq 0$) known as the **transition matrix** $P$.\n",
    "Denote a markov chain following $\\lambda$ and $P$ by $Markov(\\lambda, P)$ The entry $(P){ij}=p_{ij}$ (in row $i$ and column $j$) represents the probability of transitioning from state $i$ to state $j$.\n",
    "\n",
    "### Probabilities on a Markov Chain\n",
    "Below are the basic probability rules governing a basic Markov chain\n",
    "\\begin{array}{rl}\n",
    "    \\mathbb{P}(X_0=i_0,...,X_n=i_n) &= \\lambda_{i_0}p_{i_0i_1}...p_{i_{n-1}i_n} \\\\\\\\\n",
    "    \\mathbb{P}(X_n=i_n|X_0=i_0,...,X_{n-1}=i_{n-1})&=\\mathbb{P}(X_n=i_n|X_{n-1}=i_{n-1})=p_{i_{n-1},i_n} \\text{ (Markov property)} \\\\\\\\\n",
    "    \\mathbb{P}(X_2=j|X_0=i) &= \\sum_k{p_{ik}p_{kj}}=(P^2)_{ij} \\text{ (conditioned 2-step transition)} \\\\\\\\\n",
    "    \\mathbb{P}(X_2) &= \\sum_{i}\\lambda_i\\mathbb{P}(X_2=j|X_0=i) \\text{ (2-step transition)} \\\\\\\\\n",
    "    \\mathbb{P}(X_n=j|X_0=i) &= \\sum_{i_1,...,i_n}p_{ii_1}...p_{i_{n-1}i_n} = (P^n)_{ij} \\text{ (conditioned n-step transition)} \\\\\\\\\n",
    "    \\mathbb{P}(X_n=j) &= \\sum_{i_0,...,i_n}\\lambda_{i_0}p_{i_0i_1}...p_{i_{n-1}i_n} = (\\lambda P^n)_j \\text{ (n-step transition)} \\\\\\\\\n",
    "    (P^{n+m})_{ij} &= \\sum_k{P_{ik}^nP_{kj}^m} \\text{ (Chapman Kolmogorov equation)}\n",
    "\\end{array}\n",
    "\n",
    "### Stationary Distributions\n",
    "\n",
    "\n",
    "### Absorbing states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24561a1e",
   "metadata": {},
   "source": [
    "# CTMC [[src]()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5c53fc",
   "metadata": {},
   "source": [
    "# Dynamic Progamming principle (DPP) for problem solving\n",
    "### Definition of DPP and structure of DPP-able problems\n",
    "\n",
    "### Common DPP number sequences \n",
    "Binomial\n",
    "\n",
    "Fibonacci\n",
    "\n",
    "Catalan numbers\n",
    "\n",
    "Bell numbers\n",
    "\n",
    "Stirling numbers\n",
    "\n",
    "Motzkin numbers\n",
    "\n",
    "Schroder numbers\n",
    "\n",
    "Derangements\n",
    "\n",
    "Partition numbers\n",
    "\n",
    "### Applying DPP to Expectations\n",
    "\n",
    "\n",
    "### Applying DPP to Probabilities\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
