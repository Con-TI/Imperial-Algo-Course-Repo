{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6f3d508",
   "metadata": {},
   "source": [
    "Note: In the following equations, $tanh$ is just used as a default non-linear activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccc265f",
   "metadata": {},
   "source": [
    "## RNNs (Recurrent Neural Net)\n",
    "A neural net that incorporates a hidden state for past information at the previous time step. The primary part of the RNN is the hidden state. The output is then a function of said hidden state which does not necessarily need to be a single matrix multiplication with bias added.\n",
    "\n",
    "Evolution of the RNN:\n",
    "\\begin{array}{rl}\n",
    "h_t&=tanh(W_{xh}x_t+W_{hh}h_{t-1}+b_h)\\\\\n",
    "y_t&=W_{hy}h_t+b_y \\text{ (Toy example equation for output)}\n",
    "\\end{array}\n",
    "where \n",
    "- $y_t$ is the output at time $t$\n",
    "- $h_t$ is the hidden state at time $t$\n",
    "- $x_t$ is the data at time $t$\n",
    "- $b_{...}$ is a bias vector\n",
    "- $W_{...}$ is a weight matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a32dd1",
   "metadata": {},
   "source": [
    "## GRUs\n",
    "An RNN that uses a gating mechanism. This mechanism helps fix 2 issues: vanishing gradients, and short term memory bias. The gates control gradient flow, and allow for the hidden state to be updated only if there is sufficient information.\n",
    "\n",
    "Evolution of the GRU:\n",
    "\\begin{array}{rl}\n",
    "z_t &= \\sigma(W_zx_t+U_zh_{t-1}+b_z) \\\\\n",
    "r_t &= \\sigma(W_rx_t+U_rh_{t-1}+b_r) \\\\\n",
    "\\tilde{h}_t &= tanh(Wx_t + U(r_t\\odot h_{t-1}) + b) \\\\\n",
    "h_t &= (1-z_t)\\odot h_{t-1}+z_t\\odot h_t\n",
    "\\end{array}\n",
    "where\n",
    "- $z_t$ is the update gate with values from $0$ to $1$\n",
    "- $r_t$ is the reset gate with values from $0$ to $1$\n",
    "- $\\tilde{h}_t$ is the candidate hidden state\n",
    "- $h_t$ is the final hidden state\n",
    "- $\\sigma(\\cdot)$ is the sigmoid activation function\n",
    "- $W_{...}$ and $U_{...}$ are weight matrices\n",
    "- $b_{...}$ are bias vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257c62c0",
   "metadata": {},
   "source": [
    "## LSTMs\n",
    "An RNN that uses a memory cell as well as a gating mechanism. The additions fix issues of learning long-term dependencies, have more gates than the GRU, and separates the hidden state and memory rather than merging them like in a GRU.\n",
    "\n",
    "Evolution of the LSTM:\n",
    "\\begin{array}{rl}\n",
    "    f_t &= \\sigma(W_fx_t+U_fh_{t-1}+b_f)\\\\\n",
    "    i_t &= \\sigma(W_ix_t+U_ih_{t-1}+b_i)\\\\\n",
    "    o_t &= \\sigma(W_ox_t+U_oh_{t-1}+b_o)\\\\\n",
    "    \\tilde{c}_t &= tanh(W_cx_t+U_ch_{t-1}+b_c)\\\\\n",
    "    c_t &= f_t\\odot c_{t-1} + i_t\\odot \\tilde{c}_t\\\\\n",
    "    h_t &= o_t\\odot tanh(c_t)\n",
    "\\end{array}\n",
    "where \n",
    "- $f_t$ is the forget gate with values $0$ to $1$\n",
    "- $i_t$ is the input gate with values $0$ to $1$\n",
    "- $o_t$ is the output gate with values $0$ to $1$\n",
    "- $\\tilde{c}_t$ is the candidate memory cell vector\n",
    "- $c_t$ is the updated memory cell vector\n",
    "- $h_t$ is the hidden state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f1ba01",
   "metadata": {},
   "source": [
    "## Bi-RNNs\n",
    "An RNN that processes a sequence both forwards and backwards, used for when an entire sequence is available e.g. for text sentiment-classification.\n",
    "\n",
    "\\begin{array}{rl} \n",
    "\\overrightarrow{h}_t&=tanh(\\overrightarrow{W}_{xh}x_t+\\overrightarrow{W}_{hh}\\overrightarrow{h}_{t-1}+\\overrightarrow{b})\\\\\n",
    "\\overleftarrow{h}_t&=tanh(\\overleftarrow{W}_{xh}x_t+\\overleftarrow{W}_{hh}\\overleftarrow{h}_{t-1}+\\overleftarrow{b})\\\\\n",
    "h_t &= concat(\\overrightarrow{h}_t,\\overleftarrow{h}_t)\n",
    "\\end{array}\n",
    "where \n",
    "- $\\overrightarrow{h}_t$ is the forward hidden state\n",
    "- $\\overleftarrow{h}_t$ is the backward hidden state\n",
    "\n",
    "*The same kind of equations (a forward model and backward model) appear for the **BiLSTM** and **BiGRU**."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
