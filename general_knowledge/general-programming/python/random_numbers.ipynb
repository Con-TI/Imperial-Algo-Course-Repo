{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d70dca10",
   "metadata": {},
   "source": [
    "This notebook contains notes on how random number generation in python works"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a61e4b",
   "metadata": {},
   "source": [
    "# Introduction to randomness in Python\n",
    "### Pseudo Random numbers, time, the Mersenne twister and random.seed\n",
    "Random numbers in general are generated using **pseudo random number generators**, where **pseudo** refers to the fact that these generators are deterministic in how they produce the numbers. (Note that there is a formal theoretical CS definition for pseudorandom distributions, though this is not necessary for understanding).\n",
    "\n",
    "The generator that underlies Python (by default) is the **mersenne twister**. It involves maintaining an array of 624 32-bit integers which constantly gets updated and refreshed according to **twist** and **temper** operations. The end result of the underlying algorithm is a sequence which only repeats after $2^{19937}−1$ numbers.\n",
    "\n",
    "```random.seed(num)``` (and similar notations with ```seed()```) allows a program to reproduce random number sequence. It initializes the state array of the running script's mersenne twister based on num.\n",
    "```python\n",
    "random.seed(42)        # Set the seed\n",
    "print(random.random()) # Always outputs 0.6394267984578837\n",
    "print(random.random()) # Always outputs 0.025010755222666936\n",
    "\n",
    "random.seed(42)        # Reset the seed\n",
    "print(random.random()) # Output repeats: 0.6394267984578837\n",
    "```\n",
    "\n",
    "### From Distributions\n",
    "**$Unif(0,1)$ continuous distribution**:\n",
    "Python generates uniform random numbers within $[0,1)$ (using ```random.random()```) by calling on the mersenne twister to get a 32-bit integer then dividing the value by $2^{32}$ (effectively normalizing it to the 0-1 interval). For other ranges $[a,b]$, we can implement the following:\n",
    "```python\n",
    "def unif(a,b):\n",
    "    x = random.random()\n",
    "    # Use scaling and shifting\n",
    "    return a + x*(b-a)\n",
    "```\n",
    "\n",
    "**Uniform discrete distribution**:\n",
    "Python generates discrete uniform random numbers within arbitrary ranges $[a,b]$ by essentially using the following:\n",
    "```python\n",
    "# random.randint(a,b) is the actual function\n",
    "def randint(a,b):\n",
    "    x = random.random()\n",
    "    # equivalent to a + floor(u*(b-a+1))\n",
    "    # below implementation takes advantage\n",
    "    # of int being essentially a floor\n",
    "    # since it just truncates\n",
    "    return a + int(u*(b-a+1))\n",
    "```\n",
    "\n",
    "**Normal distribution**:\n",
    "Python generates normally distributed random variables using the ```random.normalvariate(mu,sigma)``` function where mu is the mean and sigma is the standard deviation. Underyling this function is the **box-muller transform** which uses a uniform random number generators in order to create two independent normally distributed random variables.\n",
    "```python\n",
    "# Generating a N(0,1) variable\n",
    "u1 = random.random()\n",
    "u2 = random.random()\n",
    "r = math.sqrt(-2.0 * math.log(u1))\n",
    "theta = 2.0 * math.pi * u2\n",
    "z0 = r * math.cos(theta)\n",
    "# z1 = r * math.sin(theta)\n",
    "\n",
    "# Scaling and shifting to fit mu and sigma\n",
    "ans = mu + sigma * z0\n",
    "```\n",
    "To derive the box-muller transform, consider the joint pdf of two $N(0,1)$ random variables in the $(x,y)$-coordinate system then convert into the polar coordinate system $(r,theta)$.\n",
    "\n",
    "**Inverse CDF technique**:\n",
    "For distributions whose cumulative distribution functions have an analytic inverse $F^{-1}(x)$ (where $F:[0,1]\\rightarrow \\text{distribution range}$), generating a sequence of random samples from said distributions is simply a matter of coding the inverse CDF formula $F^{-1}(x)$ then plugging in a uniform random variable from ```random.random()```.\n",
    "\n",
    "**Rejection sampling**:\n",
    "Rejection sampling is used when you can't invert the CDF easily, but for which you know the pdf function $f(x)$. The idea is to use a simpler distribution which you can sample from and satisfies:\n",
    "\\begin{equation*}\n",
    "\\forall x, f(x) \\leq Mg(x)\n",
    "\\end{equation*}\n",
    "where $f$ and $g$ are pdfs, $f$ being the target and $g$ being the one we are sampling from, and $M$ is a constant known as the envelope constant.\n",
    "\n",
    "The procedure is to then sample $x$ from the simpler distribution, and $u$ from Unif(0,1), and only accept $x$ if $u\\leq\\frac{f(x)}{Mg(x)}$ which occurs with probability $\\frac{f(x)}{Mg(x)}$. \n",
    "\n",
    "```python\n",
    "# Basic example\n",
    "# Lets say rand10() generates ints from 1 to 10 inclusive\n",
    "# You want to generate ints from 1 to 3\n",
    "# Then you can use the following\n",
    "def rand3():\n",
    "    x = rand10()\n",
    "    while x == 10:\n",
    "        x = rand10()\n",
    "        # 1 = 1,4,7 \n",
    "        # 2 = 2,5,8\n",
    "        # 3 = 3,6,9\n",
    "    return x%3+1\n",
    "```\n",
    "\n",
    "The caveat with rejection sampling is that it can be slow. \n",
    "- For starters, there is practically no upper bound on the time it would take to get an accepted sample since you could just repeatedly draw large $u$ values that get rejected. \n",
    "- The general probability of accepting is given by $\\frac{1}{M}$ (geometric), so the expected number of iterations before acceptance is given by $M$. Thus choosing the right $g(x)$ such that $M$ is small is important for better runtimes.\n",
    "\n",
    "### From sequences\n",
    "**Random permutations/shuffling**:\n",
    "Suppose we are given an array of length $n$ and we want to create a function to generate a random permutation of it such that the output has a uniform distribution over all possible permutations. Then what we want is to find a way to make an algorithm that chooses one element at a time to fill in positions in an n-array as one would if when generating a random permutation by hand. The **fisher-yates shuffle** is an algorithm that does exactly this but in linear time ($O(n)$). When it comes to shuffling an array in place, the same fisher-yates algorithm is still used.\n",
    "\n",
    "```python\n",
    "# Generating a random permutation of n total items\n",
    "# where the distribution over all possible \n",
    "# permutations is uniform\n",
    "def generate_0(n):\n",
    "    # Worst case time complexity : O(n^2)\n",
    "    arr = list(range(n))\n",
    "    perm = []\n",
    "    while arr:\n",
    "        i = random.randint(0,len(arr)-1)\n",
    "        perm.append(arr.pop(i))\n",
    "    return perm\n",
    "\n",
    "def generate_1(n):\n",
    "    # Upper bound on time complexity: infinite\n",
    "    # Expected time complexity: nlogn\n",
    "    arr = []\n",
    "    while len(arr) < n:\n",
    "        num = random.randint(1,n)\n",
    "        if num not in arr:\n",
    "            arr.append(num)\n",
    "    return arr\n",
    "\n",
    "def generate_2(n):\n",
    "    # Fisher yates shuffle\n",
    "\n",
    "    # Instead of constructing a list\n",
    "    # we modify an array in place\n",
    "    # and we \"set\" values by going from the end and choosing one of the indexes to swap with before\n",
    "    # locking in the value.\n",
    "\n",
    "    # Time complexity: n\n",
    "    arr = list(range(n))\n",
    "    for i in range(n-1, 0, -1):\n",
    "        j = random.randint(0, i)\n",
    "        arr[i], arr[j] = arr[j], arr[i]\n",
    "    return arr\n",
    "\n",
    "def generate(r,n)\n",
    "    # Truncated fisher yates shuffle but where we choose a \n",
    "    # random subset of r elements from the range 0 to n-1 to shuffle\n",
    "    arr = list(range(n))\n",
    "    for i in range(n-1, n-r-1, -1):\n",
    "        j = random.randint(0, i)\n",
    "        arr[i], arr[j] = arr[j], arr[i]\n",
    "    return arr[-r:]\n",
    "```\n",
    "\n",
    "**Random choices (i.e. n choose r)**:\n",
    "Suppose we are given an array of length $n$ and want to choose a combination (no regard for order) of $r$ items using a function. This function must generate each combination with equal probability.\n",
    "\n",
    "```python\n",
    "def generate(r,n):\n",
    "    # Brute force\n",
    "    chosen = set()\n",
    "    while len(chosen) < r:\n",
    "        chosen.add(random.randint(0, n-1))\n",
    "    return tuple(sorted(list(chosen)))\n",
    "\n",
    "def generate(r,n):\n",
    "    # Truncated fisher yates shuffle\n",
    "    arr = list(range(n))\n",
    "    for i in range(n-1, n-r-1, -1):\n",
    "        j = random.randint(0, i)\n",
    "        arr[i], arr[j] = arr[j], arr[i]\n",
    "    return tuple(sorted(arr[-r:]))\n",
    "```\n",
    "\n",
    "Suppose we assign weights to each of the items in the n-array to show the relative probabilities of choosing each element.\n",
    "```python\n",
    "# Choosing r elements from the n-array via Efraimidis–Spirakis algorithm\n",
    "def generate(r,n,weights):\n",
    "    items = list(range(n))\n",
    "    keys = []\n",
    "    for item, w in zip(items, weights):\n",
    "        # Key = -log(U)/w, U ~ Uniform(0,1)\n",
    "        u = random.random()\n",
    "        key = -math.log(u) / w # key is distributed ~ Exp(w)\n",
    "        keys.append((key, item))\n",
    "        # By the exponential weights, you can prove that P(X_i is the first) = w_i/(total w)\n",
    "        # which is exactly what you want.\n",
    "\n",
    "    # Step 2: Select top-r items by key\n",
    "    keys.sort()  # sort ascending\n",
    "    selected = [item for key, item in keys[:r]]\n",
    "    \n",
    "    return selected\n",
    "```\n",
    "**Random derangements**:\n",
    "A derangement is a permutation with one cycle, i.e. everything is out of place (nothing is in its original index). To generate derangements, we use sattolo's algorithm, which is simply the fisher-yates shuffle but we make sure not to choose the current element in the swap. This is more clearly understood by reading the code below.\n",
    "\n",
    "```python\n",
    "    # Sattolo's algorithm to generate a derangement (only one cycle). \n",
    "    def generate(n):\n",
    "        arr = list(range(n))\n",
    "        for i in range(n-1, 0, -1):\n",
    "            j = random.randint(0, i-1)  # main difference with fisher yates: j<i so the current element has to be different from the index.\n",
    "            arr[i], arr[j] = arr[j], arr[i]\n",
    "        return arr\n",
    "```\n",
    "\n",
    "### Common incorrect algorithms/mistakes and their explanations\n",
    "Fisher yates shuffle but forwards: This leads to a non-uniform distribution over the possible permutations. The easiest way to see this is to consider the total number of shuffle sequences that are possible (it will not be divisible by the total number of permutations of an n-array for all n, so it cannot be uniformly distributed by the pigeonhole principle).\n",
    "```python\n",
    "def generate(r,n)\n",
    "    arr = list(range(n))\n",
    "    for i in range(n):\n",
    "        j = random.randint(0, i)\n",
    "        arr[i], arr[j] = arr[j], arr[i]\n",
    "    return arr\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c43ab5",
   "metadata": {},
   "source": [
    "# On reservoir sampling\n",
    "**Reservoir sampling** is the process of doing $n$ choose $k$ items, but we have our $n$ as an unknown. In other words we recieve the data array $[x_1,x_2,...]$ element by element and we need an algorithm to choose $k$ items such that every possible combination has equal probability of occuring.\n",
    "\n",
    "```python\n",
    "# Single element (k=1)\n",
    "def generate(arr):\n",
    "    # In the end every element has probability 1/N of being chosen\n",
    "    # to derive the above result, consider the probability of arriving and of being replaced.\n",
    "    reservoir = arr[0]\n",
    "    # The below uses a for loop with the len(arr) but we can also use a python generator\n",
    "    for i in range(1,len(arr)):\n",
    "        x = arr[i]\n",
    "        u = random.random()\n",
    "        # probability 1/i of switching out the current element\n",
    "        if u < 1/i:\n",
    "            reservoir.pop()\n",
    "            reservoir.append(x)\n",
    "    return reservoir\n",
    "\n",
    "# k elements\n",
    "def reservoir_sample(stream, k):\n",
    "    # Overall probability that any element appears in the array is k/N\n",
    "    # to derive the above result, consider the probability of arriving and of being replaced.\n",
    "    reservoir = []\n",
    "    # Fill the reservoir with first k items\n",
    "    for i, item in enumerate(stream):\n",
    "        if i < k:\n",
    "            reservoir.append(item)\n",
    "        else:\n",
    "            # Random integer between 0 and i\n",
    "            j = random.randint(0, i)\n",
    "            if j < k:\n",
    "                reservoir[j] = item\n",
    "    return reservoir\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15c0da0",
   "metadata": {},
   "source": [
    "# Basic intro to testing and validating \"random\" generating functions\n",
    "### Rough/Visual approach: Histogram, QQ-plots and scatter plots\n",
    "Testing accuracy:\n",
    "- Histogram: A histogram lets you see whether the distribution of the output of your random function matches $f(x)$.\n",
    "- QQ-plot: A qq-plot stands for quantile-quantile and plots the empirical quantiles of a bunch of samples from your random function to $f(x)$. This works for functions which are meant to generate values on some interval rather than say a random permutation of an array. \n",
    "\n",
    "Testing randomness:\n",
    "- Scatter plot: A scatter plot of the samples against a lagged version of the samples (e.g. the samples at index $i$ against index $i-1$), can let you observe any autocorrelation between consecutive samples. There should be no obvious linear relationship in the scatterplot since samples are meant to be iid of one another.\n",
    "\n",
    "### Rigorous approach: Hypothesis testing\n",
    "General approach for hypothesis testing: Lets say your random number generator is meant to draw from distribution $f(x)$. To construct a hypothesis test on whether your generator actually draws from $f(x)$, we choose a test statistic, e.g. like the sample mean, variance, etc, then calculate the p-value under the null hypothesis that we do have the same distribution and draw conclusions accordingly.\n",
    "\n",
    "Common methods: \n",
    "- The easiest way is to rely on the central limit theorem (of course enforcing that we know the underlying has finite variance and expectation), to perform z and t-tests.\n",
    "    - z-test: If the value can be assumed to be normally distributed and with known variance. We calculate the p-value based on the probability that they lie in certain sections of the normal distribution.\n",
    "    - t-test: If the value can be assumed to be normally distributed but with an unknown variance. We calculate the p-value based on the probability that they lie in certain sections of the student t distribution.\n",
    "    - chi-square test: Mainly concerned with testing variance. Also relies on the central limit theorem.\n",
    "- A more robust way, especially if we are dealing with smaller number of samples (possibly because it takes a long time for the algorithm to generate a single sample) or a test statistic whose distribution converges too slowly to the normal distribution, would be non parametric approaches that don't assume anything about the underlying\n",
    "    - Bootstrapping: Generate many resampled datasets (with replacement) from the observed data, compute the test statistic for each, and use this empirical distribution to compute the p-value.\n",
    "    - Permutation tests: Randomly shuffle or resample labels/signs under the null hypothesis, recompute the statistic for each shuffle, and use the resulting distribution to compute the p-value. It often requires some underlying assumptions about the distribution's shape, for instance if the test was for a mean then the distribution should be symmetric about the mean.\n",
    "\n",
    "Example of looking at the mean: \n",
    "Suppose we have a random generating algorithm ```algo(lambda)``` which should output an exponentially distributed number with rate parameter lambda. Then we can let our null hypothesis be that the mean should be $1/lambda$ if it is distributed exponentially under rate parameter lambda (based on the law of large numbers). By the central limit theorem, the mean will be approximately normal distributed for a large enough number of samples. This assumption can be used to perform z and t-tests as in the below code:\n",
    "\n",
    "```python\n",
    "lambda_val = 1\n",
    "compare = 1/lambda_val # H0: mu = compare\n",
    "# Two sided one sample z-test\n",
    "def generate_p_val():\n",
    "    values = [algo(lambda_val) for i in range(1000)]\n",
    "    mu = sum(values)/len(values) # Sample mean\n",
    "    \n",
    "    assumed_sigma = (1/lambda_val)/np.sqrt(1000) # Assumed variance from H0\n",
    "\n",
    "    z_statistic = (mu - compare) / assumed_sigma # Scales according to standard normal\n",
    "\n",
    "    # probability that we get mu if the mean is distributed by N(1/lambda,(1/lambda)^2/1000)\n",
    "    p_val = 2*(1-scipy.stats.norm.cdf(abs(z_statistic))) # the farther away mu is from compare, the smaller the p_val\n",
    "    return p_val\n",
    "\n",
    "# Two sided one sample t-test\n",
    "def generate_p_val2():\n",
    "    values = [algo(lambda_val) for i in range(1000)]\n",
    "    mu = sum(values)/len(values) # Sample mean\n",
    "    sigma = np.std(values) # Sample variance\n",
    "\n",
    "    t_statistic = (mu-compare) / (sigma/np.sqrt(1000))\n",
    "    p_val = 2*(1-scipy.stats.t.cdf(abs(t_statistic), df = 999)) # degrees of freedom = 999\n",
    "    return p_val\n",
    "\n",
    "# Bootstrap confidence interval\n",
    "def generate_p_val3():\n",
    "    values = [algo(lambda_val) for i in range(1000)]\n",
    "    mu = np.mean(values)\n",
    "    \n",
    "    # Bootstrap resampling\n",
    "    boot_means = []\n",
    "    for _ in range(1000):\n",
    "        resample = np.random.choice(values, size=len(values), replace=True)\n",
    "        boot_means.append(np.mean(resample))\n",
    "    \n",
    "    boot_means = np.array(boot_means)\n",
    "    \n",
    "    # Compute two-sided p-value\n",
    "    # Probability of getting a mean at least as extreme as mu under H0\n",
    "    diff = abs(mu - compare)\n",
    "    p_val = np.mean(abs(boot_means - compare) >= diff)\n",
    "    return p_val\n",
    "\n",
    "# Permutation test\n",
    "def generate_p_val4():\n",
    "    centered = np.array(values) - compare\n",
    "    T_obs = np.mean(centered)  # observed test statistic\n",
    "    \n",
    "    perm_means = []\n",
    "    for _ in range(num_perm):\n",
    "        signs = np.random.choice([-1, 1], size=len(centered))\n",
    "        perm_means.append(np.mean(centered * signs))\n",
    "    \n",
    "    perm_means = np.array(perm_means)\n",
    "    \n",
    "    # Two-sided p-value\n",
    "    p_val = np.mean(np.abs(perm_means) >= abs(T_obs)) # If it is the mean should lead to approximately 0.5\n",
    "    return p_val\n",
    "```\n",
    "\n",
    "Example of autocorrelations: \n",
    "Suppose now that we want to make sure the random generator ```algo(lambda)``` doesn't create autocorrelated outputs. We can perform the below statistical tests:\n",
    "- Ljung box (looks at autocorrelation up to lag m)\n",
    "- Durbin watson (looks at lag 1 autocorrelation)\n",
    "- Runs test (non parametric)\n",
    "\n",
    "### Sidenote on rate of convergence to a normal distribution:\n",
    "The central limit theorem states that the mean of samples taken from a finite variance and mean distirbution converges in distribution to the normal distribution $N(\\bar{X},\\sigma^2/n)$.\n",
    "\n",
    "The **berry-esseen theorem** gives an upper bound on how far the sampling distribution of the mean is from the normal:\n",
    "\\begin{equation*}\n",
    "\\sup_x{|F_n(x)-\\Phi(x)|}\\leq\\frac{0.4748*skew}{\\sigma^3\\sqrt{n}}\n",
    "\\end{equation*}\n",
    "where $F_n(x)$ is the cdf of the standardized sample mean of n samples,  $\\sigma$ is the standard deviation of the underlying distribution and $skew$ is the skew of X (the distribution each sample is assumed to follow).\n",
    "\n",
    "Using the bound above, we can determine the value of $n$ which will make the error more or less negligible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e892be",
   "metadata": {},
   "source": [
    "# Introduction to Monte carlo\n",
    "### Basic Monte Carlo procedure\n",
    "**Monte carlo methods** refer to any method that estimates values by random sampling, e.g. estimating means, partial derivatives, integrals, etc. Suppose we have the known target distribution $p(x)$ whose expected value we want to calculate. Then we can **directly** sample from $p(x)$ repeatedly to get $n$ samples and calculate the sample mean as our estimate for the expected value. A common extension of this is the idea that you want to generate some sort of stochastic process $X_t$ up till time $T$ for which you can easily sample the distribution of the incremental changes $dX$.\n",
    "\n",
    "### MCMC procedure\n",
    "Note that below, all references to distribution $p(x)$ means that $p(x)$ is the pdf of our target distribution.\n",
    "\n",
    "**Markov chain monte carlo** is a group of monte carlo methods which are used when sampling directly from the known target distribution $p(x)$ is difficult to do. The general idea is to create a markov chain $X_0,...,X_n$ with stationary distribution $p(x)$:\n",
    "- $X_{t+1}$ depends on $X_t$ (hence a markov chain) and the sequence is generated according to bayes rule on prior and posterior distributions.\n",
    "- After some time $B$ called the **burn-in period**, the samples $X_i$ approximately $\\sim p(x)$. \n",
    "- We continue simulating the markov chain past the burn-in period and take the samples after that. \n",
    "- To reduce autocorrelation between samples from the MCMC, we can pick every k-th value after the burn in period. This is called **thinning**\n",
    "\n",
    "**Metropolis Hastings**: In the metropolis hastings algorithm, we have a known target $p(x)$ that is hard to sample from and we choose some proposal distribution $q(x|y)$. The proposal distribution is typically something that is \n",
    "- easy to sample from/generate \n",
    "- has the same support as $p(x)$ (i.e. outputs the same range of values)\n",
    "- and has the appropriate variance (tradeoff: high variance leads to more rejections and low variance leads to slow exploration and high autocorrelation).\n",
    "\n",
    "Algorithm steps:\n",
    "- initialization: start with $x_0$\n",
    "- proposal step: get $x^*$ from distribution $q(x*|x_t)$ at every time $t+1$.\n",
    "- acceptance step: accept $x*$ (i.e. update $x_t+1 = x^*$) with probability $\\alpha=\\min(1,\\frac{p(x*)q(x_t|x*)}{p(x_t)q(x*|x_t)})$. This formula can be derived by thinking about the balance between transition probabilities and $p(x)$.\n",
    "\n",
    "\n",
    "```python\n",
    "# Known pdf of the target distribution\n",
    "def p(x):\n",
    "    return np.exp(-x**4 + x**2)\n",
    "\n",
    "# we assume x* ~ N(x_t,sigma^2) where sigma is given. This is called the random walk proposal distribution and has the advantage of alpha reducing to just p(x_star)/p(x).\n",
    "def metropolis_hastings(p, x0=0.0, n_iter=10000, proposal_std=1.0):\n",
    "    samples = []\n",
    "    x = x0\n",
    "    for _ in range(n_iter):\n",
    "        # propose new sample\n",
    "        x_star = np.random.normal(x, proposal_std)\n",
    "        # acceptance ratio\n",
    "        alpha = min(1, p(x_star)/p(x))\n",
    "        # accept or reject\n",
    "        if np.random.rand() < alpha:\n",
    "            x = x_star\n",
    "        samples.append(x)\n",
    "    return np.array(samples) # no burn-in or thinning considered here, just the basic form.\n",
    "```\n",
    "\n",
    "**Gibbs sampling**: Suppose that we have a multivariate random variable which we want to simulate. The gibbs sampling procedure essentially does the same thing as metropolis hastings except we have the analytic formula for the conditional pdf for every variable given the others and always accept the proposal.\n",
    "\n",
    "```python\n",
    "# Example gibbs for bivariate normal with correlation rho\n",
    "def gibbs_sampling(rho=0.9, n_iter=10000):\n",
    "    samples = []\n",
    "    x, y = 0.0, 0.0  # start at origin\n",
    "\n",
    "    for _ in range(n_iter):\n",
    "        # sample x given y\n",
    "        mean_x = rho * y\n",
    "        std_x = np.sqrt(1 - rho**2)\n",
    "        x = np.random.normal(mean_x, std_x)\n",
    "\n",
    "        # sample y given x\n",
    "        mean_y = rho * x\n",
    "        std_y = np.sqrt(1 - rho**2)\n",
    "        y = np.random.normal(mean_y, std_y)\n",
    "\n",
    "        # Skip acceptance probability step\n",
    "        samples.append([x, y])\n",
    "\n",
    "    return np.array(samples)\n",
    "```\n",
    "\n",
    "**Hamiltonian Monte Carlo**: This is an \"extension\" of sorts of the regular metropolis hastings algorithm. Recall that for MH, we need to choose the right variance of our proposal distribution, and that in general we can have very slow convergence. HMC essentially fixes this by making big steps when we are still far from the target. Note that the notation below considers that we may have a multivariate distribution, so matrices and transposes are involved.\n",
    "\n",
    "Suppose our target pdf is given by $p(x)$. Then if we define $U(x)=-\\log p(x)$, then $p(x)=\\frac{1}{Z}e^{-U(x)}$ where Z is some normalizing constant. We start with some initial \"position\" and \"momentum\" $(x_t,p_t)$ then propose a new state $(x^*,p^*)$ according to hamiltonian dynamics:\n",
    "\\begin{array}{rl}\n",
    "    p &\\leftarrow p-\\epsilon/2 \\nabla U(x) \\\\\\\\\n",
    "    x &\\leftarrow x+\\epsilon M^{-1} p \\\\\\\\\n",
    "    p &\\leftarrow p-\\epsilon/2 \\nabla U(x)\n",
    "\\end{array}\n",
    "where $\\epsilon$ is the step size (a constant) for our dynamics and $\\nabla U(x)$ is the gradient of $U$.\n",
    "\n",
    "Afterwards, we calculate the acceptance probability:\n",
    "\\begin{array}{rl}\n",
    "H(x,p) &= U(x)+\\frac{1}{2}p^TM^{-1}p \\\\\\\\\n",
    "\\alpha &= \\min (1,e^{-H(x^*,p^*)+H(x_t,p_t)}) \n",
    "\\end{array}\n",
    "\n",
    "```python\n",
    "# Example HMC for a normal distribution.\n",
    "def U(x):\n",
    "    return 0.5 * x**2\n",
    "\n",
    "def grad_U(x):\n",
    "    return x\n",
    "\n",
    "def hmc(U, grad_U, x0, n_samples=5000, L=20, epsilon=0.1):\n",
    "    samples = []\n",
    "    x = x0\n",
    "\n",
    "    for _ in range(n_samples):\n",
    "        p = np.random.normal(0, 1)\n",
    "\n",
    "        # (x_t,p_t)\n",
    "        x_current = x\n",
    "        p_current = p\n",
    "\n",
    "        # Generate proposal state\n",
    "        p -= 0.5 * epsilon * grad_U(x)\n",
    "        for _ in range(L):\n",
    "            x += epsilon * p\n",
    "            if _ != L-1:\n",
    "                p -= epsilon * grad_U(x)\n",
    "        p -= 0.5 * epsilon * grad_U(x)\n",
    "\n",
    "        # Accept-reject\n",
    "        if np.random.rand() < np.exp((U(x_current) + 0.5 * p_current**2) - (U(x) + 0.5 * p**2)):\n",
    "            samples.append(x)\n",
    "        else:\n",
    "            x = x_current\n",
    "            samples.append(x)\n",
    "\n",
    "    return np.array(samples)\n",
    "```\n",
    "\n",
    "### Quasi Monte Carlo\n",
    "**Quasi monte carlo** methods are just like monte carlo methods in that we use sampling of many points to approximate values, particularly the values of integrals. However, one main difference is that quasi monte carlo methods use entirely deterministic sequences which are more evenly distributed across the space that we want to integrate over.\n",
    "\n",
    "The **sobol sequence** is one of the most commonly used sequences. It fills up the d-dimensional cube space $[0,1]^d$ uniformly such that there is no clustering of points as the sequence is generated. The error of an integral calculated using the sobol sequence QMC method is bounded by $O((logN)^d/N)$ where N is the number of points sampled.\n",
    "\n",
    "```python\n",
    "# Example\n",
    "# Function to integrate\n",
    "def f(x):\n",
    "    return np.exp(-x**2)\n",
    "\n",
    "# Number of points\n",
    "N = 1000\n",
    "\n",
    "sampler = qmc.Sobol(d=1, scramble=True)\n",
    "x_qmc = sampler.random(N)\n",
    "I_qmc = np.mean(f(x_qmc))\n",
    "\n",
    "x_mc = np.random.rand(N, 1)\n",
    "I_mc = np.mean(f(x_mc))\n",
    "\n",
    "print(f\"QMC estimate: {I_qmc}\")\n",
    "print(f\"MC estimate: {I_mc}\")\n",
    "```\n",
    "\n",
    "Sidenote: There are many other sequences you can explore, but I am not sure how useful anything beyond the sobol sequence is."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
